{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13157632,"sourceType":"datasetVersion","datasetId":8336843}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Single-cell PHI-2-only SFT + optional DPO training for Kaggle (paste whole cell)\n# Enforces microsoft/phi-2 only. Will stop early if phi-2 cannot be loaded.\n# Kaggle-safe: TOKENIZERS_PARALLELISM=false, dataloader_num_workers=0, minimal worker forks.\n\nimport os, sys, subprocess, time, json, math, random\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\n\n# ------------------ VERY EARLY ENV (must come before tokenizers/transformers imports) ------------------\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"HF_DISABLE_TELEMETRY\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n\n# ------------------ USER-TUNABLE PARAMETERS ------------------\nDATA_FILE = \"/kaggle/input/pqc-hack/dataset.jsonl\"\nMODEL_NAME = \"microsoft/phi-2\"   # STRICT: enforced below\nOUT_SFT = \"/kaggle/working/pqc-phi2-lora\"\nOUT_DPO = \"/kaggle/working/pqc-phi2-lora-dpo\"\nVAL_SPLIT = 0.05\nMAX_LENGTH = 2048\nBATCH_SIZE = 1\nGRAD_ACCUM = 16\nSFT_EPOCHS = 1\nDPO_EPOCHS = 1\nSFT_LR = 1.5e-4\nDPO_LR = 5e-6\nWARMUP_RATIO = 0.05\nLR_SCHED = \"cosine\"\nSEED = 42\nHF_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")  # set as Kaggle secret env var\nSYSTEM_PROMPT = (\"You are PQC-Guard. Stay strictly within post-quantum cryptography. \"\n                 \"If a query is outside PQC, briefly say it's out of scope.\")\nUSER_TAG = \"<|user|>\"; ASSISTANT_TAG = \"<|assistant|>\"; SYSTEM_TAG = \"<|system|>\"\nEND_TAG = \"</s>\"\nREFUSAL_TEXT = (\"I'm focused on PQC (post-quantum cryptography) topics only. \"\n                \"For PQC algorithms, migrations, KEMs, signatures, or TLS/PKI, I can help.\")\n\n# ------------------ helper: robust pip installs (minimal, Kaggle-friendly) ------------------\ndef pip_install(pkgs, retries=2):\n    for spec in pkgs:\n        attempt = 0\n        while attempt < retries:\n            try:\n                print(f\"[pip] Installing: {spec} (attempt {attempt+1})\")\n                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", spec],\n                               check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n                break\n            except subprocess.CalledProcessError as e:\n                attempt += 1\n                print(f\"[pip] Failed install attempt {attempt} for {spec}. Retrying...\")\n                time.sleep(1 + attempt)\n        else:\n            print(f\"[pip] WARNING: Could not install {spec} after {retries} attempts. Proceeding anyway.\")\n\n# minimal required packages (avoid optional heavy extras)\nto_install = [\n    \"transformers>=4.34.0\",\n    \"datasets\",\n    \"accelerate\",\n    \"peft\",\n    \"trl\",\n    \"einops\",\n    \"bitsandbytes>=0.45.3\"\n]\npip_install(to_install)\n\n# ------------------ imports (after installs) ------------------\nimport torch\nfrom datasets import Dataset\nfrom transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n                          Trainer, TrainingArguments, set_seed, TrainerCallback, TrainerState, TrainerControl)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import DPOTrainer\n\n# ------------------ environment & determinism ------------------\nset_seed(SEED)\ntorch.backends.cuda.matmul.allow_tf32 = True\nUSE_CUDA = torch.cuda.is_available()\ndevice_name = torch.cuda.get_device_name(0) if USE_CUDA else \"CPU\"\nprint(f\"[ENV] Device: {device_name} | Torch {torch.__version__} | CUDA available={USE_CUDA}\")\n\n# ------------------ load data (resilient) ------------------\nrows: List[Dict] = []\nif not os.path.exists(DATA_FILE):\n    raise FileNotFoundError(f\"Data file not found: {DATA_FILE} (set DATA_FILE correctly)\")\n\nwith open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        line = line.strip()\n        if not line:\n            continue\n        try:\n            rows.append(json.loads(line))\n        except json.JSONDecodeError:\n            continue\n\nmodes = Counter((r.get(\"response\") or {}).get(\"mode\") for r in rows)\nmissing_ans = sum(\n    1 for r in rows if not isinstance((r.get(\"response\") or {}).get(\"answer\"), str) or not (r.get(\"response\") or {}).get(\"answer\").strip()\n)\nprint(f\"[DATA] Loaded {len(rows)} rows | Mode counts: {dict(modes)} | Empty 'answer' rows: {missing_ans}\")\n\n# ------------------ prompt helpers ------------------\ndef build_prompt(instr: str, kb_refs: Optional[List[str]] = None) -> str:\n    ctx = f\"Context kb_refs: {', '.join(kb_refs)}\\n\" if kb_refs else \"\"\n    return (f\"{SYSTEM_TAG}\\n{SYSTEM_PROMPT}\\n\"\n            f\"{USER_TAG}\\nInstruction: {instr if instr else '[no instruction provided]'}\\n{ctx}\"\n            f\"{ASSISTANT_TAG}\\n\")\n\ndef row_to_text(x: Dict) -> Optional[Dict]:\n    instr = (x.get(\"instruction\") or \"\").strip()\n    ctx = x.get(\"context\") or {}\n    kb_refs = None\n    if isinstance(ctx, dict) and isinstance(ctx.get(\"kb_refs\"), list) and ctx[\"kb_refs\"]:\n        kb_refs = [str(k) for k in ctx[\"kb_refs\"]]\n    resp = x.get(\"response\") or {}\n    ans = (resp.get(\"answer\") or \"\").strip()\n    if not ans:\n        ans = REFUSAL_TEXT\n    prompt = build_prompt(instr, kb_refs)\n    full = prompt + ans + \"\\n\" + END_TAG\n    return {\"prompt\": prompt, \"answer\": ans, \"text\": full}\n\nmapped = []\nfor r in rows:\n    rec = row_to_text(r)\n    if rec:\n        mapped.append(rec)\n\nprint(\"[DATA] Mapped rows:\", len(mapped))\nfull_ds = Dataset.from_list(mapped)\nsplit = full_ds.train_test_split(test_size=VAL_SPLIT, seed=SEED) if len(full_ds) > 1 else {\"train\": full_ds, \"test\": Dataset.from_list([])}\ntrain_ds, val_ds = split[\"train\"], split[\"test\"]\n\n# ------------------ tokenizer ------------------\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, use_auth_token=HF_TOKEN)\nadded = tokenizer.add_special_tokens({\"additional_special_tokens\": [USER_TAG, ASSISTANT_TAG, SYSTEM_TAG]})\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\nprint(f\"[TOKENIZER] added {added} special tokens; pad={tokenizer.pad_token!r}\")\n\n# ------------------ tokenization with label masking ------------------\ndef tokenize_with_mask(ex):\n    prompt = ex[\"prompt\"]\n    full = ex[\"text\"]\n    ft = tokenizer(full, truncation=True, max_length=MAX_LENGTH)\n    pt = tokenizer(prompt, truncation=True, max_length=MAX_LENGTH)\n    input_ids = ft[\"input_ids\"]\n    attention_mask = ft[\"attention_mask\"]\n    labels = input_ids.copy()\n    prompt_len = len(pt[\"input_ids\"]) if isinstance(pt[\"input_ids\"], list) else len(pt[\"input_ids\"][0])\n    for i in range(min(prompt_len, len(labels))):\n        labels[i] = -100\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n# Use num_proc=1 to avoid tokenizers + map forking warnings on Kaggle\ntrain_tok = train_ds.map(tokenize_with_mask, remove_columns=train_ds.column_names, num_proc=1)\nval_tok = val_ds.map(tokenize_with_mask, remove_columns=val_ds.column_names, num_proc=1) if len(val_ds) > 0 else None\nprint(\"[TOKENIZE] Tokenized datasets:\", len(train_tok), \" / val:\", len(val_tok) if val_tok else 0)\n\n# ------------------ BitsAndBytes config ------------------\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    # compute dtype: prefer bfloat16 for Ampere+ if available; else float16\n    bnb_4bit_compute_dtype=torch.bfloat16 if (USE_CUDA and torch.cuda.get_device_capability(0)[0] >= 8) else torch.float16,\n)\n\n# ------------------ Strict model-loading (PHI-2 only) ------------------\ndef check_enforce_phi2(name: str):\n    if name != \"microsoft/phi-2\":\n        print(f\"[ENFORCE] Overriding MODEL_NAME -> 'microsoft/phi-2' (user or env had: {name})\")\n    return \"microsoft/phi-2\"\n\nMODEL_NAME = check_enforce_phi2(MODEL_NAME)\n\ndef try_load_model_phi2(name: str, use_4bit=True):\n    kwargs = {\"trust_remote_code\": True}\n    if HF_TOKEN:\n        kwargs[\"use_auth_token\"] = HF_TOKEN\n    if torch.cuda.is_available():\n        kwargs[\"device_map\"] = \"auto\"\n    try:\n        if use_4bit:\n            print(f\"[MODEL] Attempting 4-bit load for {name} ...\")\n            model = AutoModelForCausalLM.from_pretrained(name, quantization_config=bnb_config, **kwargs)\n        else:\n            print(f\"[MODEL] Attempting fp16 load for {name} ...\")\n            # fp16 fallback on same model only\n            torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n            model = AutoModelForCausalLM.from_pretrained(name, torch_dtype=torch_dtype, **kwargs)\n        # sanity: ensure model reports correct name/path in config\n        reported = getattr(model.config, \"_name_or_path\", None) or getattr(model, \"name_or_path\", None)\n        print(f\"[MODEL] Loaded model -> reported name: {reported}\")\n        if reported and \"phi-2\" not in str(reported).lower() and \"phi2\" not in str(reported).lower():\n            # if model doesn't look like phi2, raise (we refuse other models)\n            raise RuntimeError(f\"Loaded model does not look like phi-2 (reported: {reported}). Aborting by design.\")\n        return model\n    except Exception as e:\n        print(f\"[MODEL] Failed to load {name} (use_4bit={use_4bit}): {type(e).__name__}: {e}\")\n        return None\n\n# Try 4-bit first, then fp16 — but only for PHI-2. If both fail, raise and stop.\nbase_model = try_load_model_phi2(MODEL_NAME, use_4bit=True)\nif base_model is None:\n    print(\"[MODEL] 4-bit load failed, trying fp16 fallback for phi-2 (same model only).\")\n    base_model = try_load_model_phi2(MODEL_NAME, use_4bit=False)\n\nif base_model is None:\n    raise RuntimeError(\n        \"Unable to load microsoft/phi-2 on this runtime. By design we DO NOT auto-fallback to other models. \"\n        \"Common causes: insufficient GPU memory, missing HF token, or network/access issues. \"\n        \"Options: (1) ensure HUGGINGFACE_TOKEN is set and has access; (2) pick a runtime with more GPU memory; \"\n        \"or (3) manually load a smaller phi-2 variant if available. Aborting.\"\n    )\n\n# Resize embeddings to account for added tokens\nbase_model.resize_token_embeddings(len(tokenizer))\nbase_model = prepare_model_for_kbit_training(base_model, use_gradient_checkpointing=True)\nprint(\"[MODEL] prepare_model_for_kbit_training complete.\")\n\n# ------------------ LoRA config & apply ------------------\nlinear_names = {n.split(\".\")[-1] for n, m in base_model.named_modules() if isinstance(m, torch.nn.Linear)}\nCANDS = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\"Wqkv\",\"out_proj\",\"fc1\",\"fc2\"]\ntarget_modules = sorted([t for t in CANDS if t in linear_names]) or sorted(list(linear_names))[:8]\nprint(\"[LORA] LoRA targets:\", target_modules)\n\npeft_cfg = LoraConfig(\n    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=target_modules\n)\npolicy = get_peft_model(base_model, peft_cfg)\npolicy.config.use_cache = False\npolicy.config.pad_token_id = tokenizer.pad_token_id\n\n# ------------------ collator (no worker forks) ------------------\n@dataclass\nclass LMDataCollator:\n    tokenizer: any\n    def __call__(self, feats: List[Dict]):\n        import torch\n        pad_id = self.tokenizer.pad_token_id\n        ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in feats]\n        amask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in feats]\n        labs = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in feats]\n        input_ids = torch.nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=pad_id)\n        attention_mask = torch.nn.utils.rnn.pad_sequence(amask, batch_first=True, padding_value=0)\n        labels = torch.nn.utils.rnn.pad_sequence(labs, batch_first=True, padding_value=-100)\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\ncollator = LMDataCollator(tokenizer)\n\n# ------------------ Trainer progress callback (logs memory & ensures model is PHI-2) ------------------\nclass ProgressCallback(TrainerCallback):\n    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n        # Called frequently; keep output minimal but informative\n        step = state.global_step\n        if torch.cuda.is_available():\n            used = torch.cuda.memory_allocated() / (1024**2)\n            reserved = torch.cuda.memory_reserved() / (1024**2)\n            print(f\"[PROG] step={step} | cuda_used={used:.1f}MB reserved={reserved:.1f}MB\")\n            # try nvidia-smi quick query (best-effort)\n            try:\n                out = subprocess.run([\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"],\n                                     stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True, timeout=2)\n                if out.returncode == 0:\n                    print(\"[PROG] nvidia-smi:\", out.stdout.strip())\n            except Exception:\n                pass\n        else:\n            print(f\"[PROG] step={step} (CPU run)\")\n\n    def on_save(self, args, state, control, **kwargs):\n        # verify saved model is phi-2 (best-effort)\n        model = kwargs.get(\"model\") or policy\n        reported = getattr(model.config, \"_name_or_path\", None) or getattr(model, \"name_or_path\", None)\n        print(f\"[PROG] save triggered at step {state.global_step}. reported model: {reported}\")\n        if reported and \"phi-2\" not in str(reported).lower() and \"phi2\" not in str(reported).lower():\n            print(\"[PROG][WARNING] Saved model does not look like phi-2. THIS SHOULD NOT HAPPEN (enforced).\")\n\n# ------------------ SFT training (Trainer) ------------------\nos.makedirs(OUT_SFT, exist_ok=True)\nsft_args = TrainingArguments(\n    output_dir=OUT_SFT,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM,\n    num_train_epochs=SFT_EPOCHS,\n    learning_rate=SFT_LR,\n    lr_scheduler_type=LR_SCHED,\n    warmup_ratio=WARMUP_RATIO,\n    weight_decay=0.01,\n    max_grad_norm=0.3,\n    bf16=(USE_CUDA and torch.cuda.get_device_capability(0)[0] >= 8),\n    fp16=(USE_CUDA and torch.cuda.get_device_capability(0)[0] < 8),\n    logging_steps=50,\n    eval_strategy=\"steps\" if val_tok else \"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=800,\n    save_total_limit=2,\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",\n    group_by_length=True,\n    dataloader_num_workers=0,  # avoid worker forks on Kaggle\n)\n\ntrainer = Trainer(\n    model=policy,\n    args=sft_args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok if val_tok else None,\n    data_collator=collator,\n    tokenizer=tokenizer,\n    callbacks=[ProgressCallback()],\n)\n\nprint(\"[SFT] Starting SFT training (phi-2 only). If this fails due to OOM, consider using a runtime with more GPU memory.\")\ntrainer.train()\ntrainer.model.save_pretrained(OUT_SFT); tokenizer.save_pretrained(OUT_SFT)\nprint(f\"[SFT] Saved LoRA SFT -> {OUT_SFT}\")\n\n# evaluate basic eval loss if available (best-effort)\ntry:\n    m = trainer.evaluate()\n    if (lv := m.get(\"eval_loss\")) is not None:\n        ppl = math.exp(lv) if lv < 20 else float(\"inf\")\n        print(f\"[SFT] eval_loss={lv:.4f}, ppl={ppl:.2f}\")\nexcept Exception as e:\n    print(\"[SFT] evaluation skipped or failed:\", e)\n\n# ------------------ DPO dataset + training (auto-skip) ------------------\npairs_flat = []\nfor r in rows:\n    p = r.get(\"preference_pair\")\n    if isinstance(p, dict) and {\"prompt\",\"chosen\",\"rejected\"} <= set(p.keys()):\n        rec = {\"prompt\": str(p[\"prompt\"]), \"chosen\": str(p[\"chosen\"]), \"rejected\": str(p[\"rejected\"])}\n        if isinstance(p.get(\"kb_refs\"), list):\n            rec[\"kb_refs\"] = p[\"kb_refs\"]\n        pairs_flat.append(rec)\n\nif len(pairs_flat) == 0:\n    print(\"[DPO] No preference pairs found → DPO skipped.\")\nelse:\n    def map_pairs(p):\n        kb = p.get(\"kb_refs\") if isinstance(p.get(\"kb_refs\"), list) else None\n        ctx = f\"Context kb_refs: {', '.join(kb)}\\n\" if kb else \"\"\n        prompt = (f\"{SYSTEM_TAG}\\n{SYSTEM_PROMPT}\\n\"\n                  f\"{USER_TAG}\\nInstruction: {p.get('prompt','')}\\n{ctx}\"\n                  f\"{ASSISTANT_TAG}\\n\")\n        return {\"prompt\": prompt, \"chosen\": str(p.get(\"chosen\",\"\")), \"rejected\": str(p.get(\"rejected\",\"\"))}\n    tmp_pairs = Dataset.from_list(pairs_flat)\n    dpo_ds = tmp_pairs.map(map_pairs, remove_columns=[c for c in tmp_pairs.column_names if c not in {\"prompt\",\"chosen\",\"rejected\"}], num_proc=1)\n    print(\"[DPO] DPO rows:\", len(dpo_ds))\n\n    # load *reference* model for DPO — must be phi-2 too. Try fp16 only for reference.\n    ref_model = try_load_model_phi2(MODEL_NAME, use_4bit=False)\n    if ref_model is None:\n        print(\"[DPO] Couldn't load phi-2 reference model for DPO; skipping DPO stage by design.\")\n    else:\n        dpo_args = TrainingArguments(\n            output_dir=OUT_DPO,\n            per_device_train_batch_size=BATCH_SIZE,\n            per_device_eval_batch_size=BATCH_SIZE,\n            gradient_accumulation_steps=GRAD_ACCUM,\n            num_train_epochs=DPO_EPOCHS,\n            learning_rate=DPO_LR,\n            lr_scheduler_type=LR_SCHED,\n            warmup_ratio=WARMUP_RATIO,\n            bf16=(USE_CUDA and torch.cuda.get_device_capability(0)[0] >= 8),\n            fp16=(USE_CUDA and torch.cuda.get_device_capability(0)[0] < 8),\n            logging_steps=50,\n            save_strategy=\"steps\",\n            save_steps=800,\n            save_total_limit=2,\n            report_to=\"none\",\n            optim=\"paged_adamw_8bit\",\n            group_by_length=True,\n            dataloader_num_workers=0,\n        )\n        dpo_trainer = DPOTrainer(model=policy, ref_model=ref_model, args=dpo_args, beta=0.1, train_dataset=dpo_ds, tokenizer=tokenizer)\n        print(\"[DPO] Starting DPO training (phi-2 reference).\")\n        dpo_trainer.train()\n        os.makedirs(OUT_DPO, exist_ok=True)\n        dpo_trainer.model.save_pretrained(OUT_DPO); tokenizer.save_pretrained(OUT_DPO)\n        print(\"[DPO] Saved LoRA DPO ->\", OUT_DPO)\n\n# ------------------ Zip outputs ------------------\nimport zipfile\ndef zip_dir(src_dir: str, zip_path: str):\n    src_dir = os.path.abspath(src_dir)\n    with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n        for root, _, files in os.walk(src_dir):\n            for f in files:\n                full = os.path.join(root, f)\n                rel = os.path.relpath(full, os.path.dirname(src_dir))\n                zf.write(full, arcname=rel)\n\nsft_zip = \"/kaggle/working/pqc-phi2-lora.zip\"\nzip_dir(OUT_SFT, sft_zip)\nprint(\"[ZIP] Zipped:\", sft_zip)\nif os.path.isdir(OUT_DPO) and any(True for _ in os.scandir(OUT_DPO)):\n    dpo_zip = \"/kaggle/working/pqc-phi2-lora-dpo.zip\"\n    zip_dir(OUT_DPO, dpo_zip)\n    print(\"[ZIP] Zipped:\", dpo_zip)\nelse:\n    print(\"[ZIP] No DPO output to zip (skipped).\")\n\nprint(\"[DONE] All done. Check /kaggle/working for outputs. NOTE: This run strictly enforces microsoft/phi-2 only (no automatic fallbacks to other models).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}